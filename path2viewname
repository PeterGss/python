import json
import tensorflow as tf
from collections import Counter
from itertools import dropwhile
import numpy as np
S = {
    'bs': 50,              #batch size 一次训练多少个模板
    'ti': 5000,            #training iterations  训练多少次
    'lr': 1e-2,             #learning rate  学习速度
    'il': 3,                #interest level, >= which would be deemed interesting
    }
tf.set_random_seed(0)     #调整随机 确保下次也能得到同样结果
np.random.seed(0)
def read_data():         #读取数据
    entries = []
    with open(r'F:/Python/pycharm/workspace/trains.jsonl', 'r') as f:
        for line in f:
            s = line.strip('\x00')
            entries.append(json.loads(s[s.rfind('\x00')+1:]))
    return entries
#主函数
def main():
    entries = read_data()
    #
    view_names = set()
    interesting_attributes = Counter()

    for i in entries:
        view_names.add(i['view_name'])
        pathdir = i['path'].split('/')
        for j in pathdir:
            interesting_attributes[j] += 1

    for key, count in dropwhile(lambda key_count: key_count[1] >= S['il'], interesting_attributes.most_common()):
        del interesting_attributes[key]
        # 把所有的view_name 加到view_names set中 并且数所有的网址后/分开的attributes   大于等于三的保留

    view_names = list(view_names)
    i2n, n2i = dict(), dict()
    #view_names 的转换
    for i, n in enumerate(view_names):
        i2n[i] = n
        n2i[n] = i

    interesting_attributes = interesting_attributes.keys()
    #path的转化
    i2a, a2i = dict(), dict()
    for i, a in enumerate(interesting_attributes):
        i2a[i] = a
        a2i[a] = i
        #创造一个转化为整形张量的一个索引，能转换过去，转换回来

    global_step= tf.Variable(0., False, dtype=tf.float32)
    #建立图
    x = tf.placeholder(tf.float32, [None, len(interesting_attributes)], name = 'input')
    y_ = tf.placeholder(tf.float32, [None, len(view_names)], name = 'label')
    W = tf.Variable(tf.random_normal([len(interesting_attributes), len(view_names)]), dtype = tf.float32)
    b = tf.Variable(tf.random_normal([len(view_names)], dtype = tf.float32))
    linear = tf.matmul(x, W) + b
    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.nn.softmax(linear), 1), tf.argmax(y_, 1)), tf.float32))
    loss = tf.losses.softmax_cross_entropy(y_, linear)
    learning_rate = tf.train.exponential_decay(S['lr'], global_step, S['ti'] // 10, 0.5, staircase = True)
    optimizer = tf.train.AdamOptimizer(learning_rate)
    train = optimizer.minimize(loss, global_step)
    init = tf.global_variables_initializer()
    #建立图
    with tf.Session() as sess:
        sess.run(init)
        for i in range(S['ti']):
            feed_indices = np.random.randint(low = 0, high = len(entries), size = S['bs'])
            feed_input_arrays = []
            feed_labels = np.array([[0.] * len(view_names) for _ in range(S['bs'])], dtype = np.float32)
            for ji, j in enumerate(feed_indices):
                ia = np.zeros(len(interesting_attributes), dtype = np.float32)
                feed_labels[ji][n2i[entries[j]['view_name']]] = 1.
                for k in entries[j]['path'].split('/')[1:]:
                    if k in interesting_attributes:
                        ia[a2i[k]] = 1.
                feed_input_arrays.append(ia)

            feed_input = np.stack(feed_input_arrays)

            l, a, _ = sess.run((loss, accuracy, train), feed_dict={x: feed_input, y_: feed_labels})
            print('batch {}/{}: accuracy = {:.2f}%, loss = {}'.format(i, S['ti'], a*100, l))#输出 精确度


if __name__ == '__main__':
    main()
